import sys
import logging
from api import client
from messages.questions import greeting, hannibal_question
from messages.few_shots import capitalize_task, format_numbers_task, \
seven_wonders_of_world_task


logger = logging.getLogger()
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)


def show_available_models():
    """Show all available models."""
    models = client.models.list()
    logger.info("Available models:")
    for model in models:
        logger.info(model.id)


def create_chat_completion(model, messages, max_tokens=50):
    """The text inputs to the models are referred to as "prompts".
    As models have no memory of past requests, all relevant information must be supplied
    as part of the conversation history in each request.
    Chat models will take a list of messages as input and return a -generated message as output.
    They are not deterministic by default, but you can set a seed or fingerprint param to make it deterministic.
    Messages input must be an array of message objects, where
    each object has a role (either "system", "user", or "assistant") and content:

    - The system message (optional) helps set the behavior of the assistant. For instance: 'You are a movie expert.'
    - The user role represents the user and is used to send a message to the model.
    - This assistant role is optional and represents the AI model and is used to send a message to the user.

    Language models read and write text in chunks called tokens.
    For example, the string "ChatGPT is great!" is encoded into six tokens: ["Chat", "G", "PT", " is", " great", "!"].
    You pay by the input + output tokens (see the total_tokens in the response).

    The length of the output is determined by the API. To control the length of the output, you can use the max_tokens parameter.
    The max_token parameter consider the prompt + model tokens. 
    This parameter is a number that depends on the model. For example, the max_tokens for the GPT-3.5-Turbo model is 4095.

    See the docs: https://platform.openai.com/docs/guides/text-generation/managing-tokens

    Returns a ChatCompletion object. For instance:

    ChatCompletion(
        id='chatcmpl-8jvdNPNv7923lcVHtn3zjNt3GqkAY',
        choices=[
            Choice(
                finish_reason='stop', # stop, length (exceeded max tokens or limit), function_call, content_filter (was flagged) or null (api has more tokens to generate)
                index=0,
                logprobs=None, # probabilities for each token
                message=ChatCompletionMessage(
                    content='Hello! How can I help you today?',
                    role='assistant',
                    function_call=None, # function used to generate it
                    tool_calls=None
                )
            )
        ],
        created=1705956997,
        model='gpt-3.5-turbo-0613',
        object='chat.completion',
        system_fingerprint=None, # backend configuration of an OpenAI model
        usage=CompletionUsage(
            completion_tokens=9, # number of tokens generated by the model (output)
            prompt_tokens=22, # input tokens
            total_tokens=31
            )
        )
    """
    logger.info(f"Chat completition with {model}:")
    return client.chat.completions.create(
        model=model, 
        messages=messages,
        max_tokens=max_tokens
        #response_format={ "type": "json_object" }
    )


def create_a_few_shot_chat_completion_task(model, messages, temperature=0.2, prefix=''):
    '''Few-shot learning is a technique that allows you to train a model on a small dataset to perform a specific task.
    For instance, capitalize the first letter of each word, except for articles, conjunctions, and prepositions.

    In this example, the api is supplied with user messagens and assistant messages to instruct the model how the 
    responses should be. At the end, there is one more user message without the assistant response, that represents
    the task that the model should complete.

    About the temperature parameter: Select a temperature value based on the desired trade-off between coherence 
    and creativity for your specific application. The temperature can range is from 0 to 2.
    Lower values for temperature result in more consistent outputs (e.g. 0.2), 
    while higher values generate more diverse and creative results (e.g. 1.0).

    The model could takes our starter (prefix) and provides a suitable continuation. 
    This powerful feature allows us to get specific outputs from the model without providing every detail; 
    the model fills in the gaps based on its training.

    '''
    logger.info(f"Few shot chat completition with {model}:")
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
    )
    return f'{prefix} {response.choices[0].message.content}'


if __name__ == "__main__":

    #show_available_models()


    logger.info(create_chat_completion("gpt-3.5-turbo", greeting).choices[0].message.content)

    logger.info(create_chat_completion("gpt-3.5-turbo", hannibal_question, max_tokens=100).choices[0].message.content)

    logger.info(create_a_few_shot_chat_completion_task("gpt-4", capitalize_task, 1.2))

    logger.info(create_a_few_shot_chat_completion_task("gpt-4", format_numbers_task))

    logger.info(create_a_few_shot_chat_completion_task("gpt-4", seven_wonders_of_world_task, prefix='\n1.'))
