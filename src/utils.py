import pandas as pd
import api
import numpy as np
from logger import Logger
import nltk


logger = Logger().get_logger()


def get_embedding(text, model="text-embedding-ada-002"):
    """Embeddings are generated by training models on massive text corpora.
    These models, such as Word2Vec, GloVe, or BERT, learn by observing how words are used in sentences.
    Through training, the model learns to associate words with their contexts.
    This means that words appearing in similar contexts are positioned closer together in the embedding space.
    """
    text = text.replace("\n", " ")
    return api.create_embedding(input=[text], model=model).data[0].embedding


def calc_cosine_similarity(a, b):
    """In the high-dimensional vector space, each word is represented as a vector.
    The distance between these points is a measure of semantic similarity.
    Techniques like cosine similarity are used to measure how close or far apart these vectors are.
    """
    numerator = np.dot(a, b)
    denominator = np.linalg.norm(a) * np.linalg.norm(b)
    return numerator / denominator


def calc_similarities(input, embeddings):
    similarities = []
    for embedding in embeddings:
        similarity = calc_cosine_similarity(input, embedding)
        similarities.append(similarity)
    return similarities


def generate_embeddings_for_dataset(raw_dataset, columns, nrows=50):
    """Generate the embeddings for dataset and save it
    to a file with extension _embeddings.csv"""

    logger.info(f"Generating embeddings for {raw_dataset}...")
    try:
        df = pd.read_csv(raw_dataset, nrows=nrows)
        for column in columns:
            df[f"{column}_preprocessed"] = df[column].apply(preprocess_text)
            df[f"{column}_embedding"] = df[f"{column}_preprocessed"].apply(
                lambda x: get_embedding(x)
            )
        output_filename = raw_dataset.replace(".csv", "_embeddings.csv")
        df.to_csv(output_filename)
        logger.info(f"Embeddings saved to {output_filename}.")
    except Exception as e:
        logger.info(f"[ERROR] {e}")
        exit()


def download_nltk_data():
    """Download the datasets and models required by the
    NLTK (Natural Language Toolkit) library:
    - tokenizers/punkt: Punkt tokenizer model (used to divide a text into a list of sentences using an unsupervised algorithm);s
    - corpora/stopwords: dataset of stopwords that are usually removed in the preprocessing step.
    """

    # Check and download the 'punkt' tokenizer models
    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        nltk.download("punkt")

    # Check and download the 'stopwords' corpus
    try:
        nltk.data.find("corpora/stopwords")
    except LookupError:
        nltk.download("stopwords")


def preprocess_text(text):
    """Preprocess text by tokenizing (splitting a text into a list of words or sentences),
    converting to lowercase, removing punctuation, removing stopwords and
    stemming (converts words to their root form. For example, running is converted to run).
    This is done to reduce the vocabulary size (an alternative is to use Lemmatization, that is
    a more accurated way to do this because it considers the grammatical rules and context).
    """
    from nltk.corpus import stopwords
    from nltk.stem import PorterStemmer
    from nltk.tokenize import word_tokenize

    download_nltk_data()

    # Tokenize text
    tokens = word_tokenize(text)

    # Convert to lower case
    tokens = [word.lower() for word in tokens]

    # Remove punctuation
    words = [word for word in tokens if word.isalpha()]

    # Filter out stop words
    words = [word for word in words if word not in set(stopwords.words("english"))]

    # Stemming
    stemmer = PorterStemmer()
    stemmed_words = [stemmer.stem(word) for word in words]

    return " ".join(stemmed_words)
